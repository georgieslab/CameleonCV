{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶é CameleonCV - LoRA Fine-Tuning\n",
        "\n",
        "This notebook fine-tunes LLaMA 3 (8B) using LoRA for style-aware CV transformation.\n",
        "\n",
        "**What you'll need:**\n",
        "- Google Colab Pro (A100 GPU recommended)\n",
        "- Training data uploaded to Google Drive\n",
        "- ~1-2 hours for training\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Installs Unsloth (efficient LoRA training library)\n",
        "2. Loads your 1,050 training examples\n",
        "3. Fine-tunes LLaMA 3 8B with LoRA adapters\n",
        "4. Saves the adapter to Google Drive\n",
        "5. Tests inference with a sample\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **Before starting:** Make sure you've selected a GPU runtime!\n",
        "\n",
        "`Runtime ‚Üí Change runtime type ‚Üí A100 GPU` (or T4 if A100 unavailable)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "This installs Unsloth (fast LoRA training) and required libraries. Takes ~2-3 minutes."
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth for efficient LoRA training\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU found! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Mount Google Drive & Load Data\n",
        "\n",
        "Upload your `train.jsonl` and `validation.jsonl` to Google Drive first!\n",
        "\n",
        "**Recommended folder structure:**\n",
        "```\n",
        "My Drive/\n",
        "  CameleonCV/\n",
        "    data/\n",
        "      train.jsonl\n",
        "      validation.jsonl\n",
        "      test.jsonl\n",
        "    outputs/\n",
        "      (adapter will be saved here)\n",
        "```"
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths - UPDATE THESE IF YOUR FOLDER STRUCTURE IS DIFFERENT\n",
        "DATA_DIR = \"/content/drive/MyDrive/CameleonCV/data\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/CameleonCV/outputs\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Data directory: {DATA_DIR}\")\n",
        "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify data files exist\n",
        "import os\n",
        "\n",
        "required_files = ['train.jsonl', 'validation.jsonl']\n",
        "for f in required_files:\n",
        "    path = os.path.join(DATA_DIR, f)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / 1024 / 1024\n",
        "        print(f\"‚úÖ {f} found ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {f} NOT FOUND at {path}\")\n",
        "        print(\"   Please upload your data files to Google Drive!\")"
      ],
      "metadata": {
        "id": "verify_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect training data\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    \"\"\"Load JSONL file into list of dicts\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "# Load datasets\n",
        "train_data = load_jsonl(os.path.join(DATA_DIR, 'train.jsonl'))\n",
        "val_data = load_jsonl(os.path.join(DATA_DIR, 'validation.jsonl'))\n",
        "\n",
        "print(f\"\\nüìä Dataset loaded:\")\n",
        "print(f\"   Training examples: {len(train_data)}\")\n",
        "print(f\"   Validation examples: {len(val_data)}\")\n",
        "\n",
        "# Show distribution\n",
        "styles = Counter(ex['metadata']['target_style'] for ex in train_data)\n",
        "print(f\"\\nüìà Style distribution in training set:\")\n",
        "for style, count in sorted(styles.items()):\n",
        "    print(f\"   {style}: {count}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview one training example\n",
        "example = train_data[0]\n",
        "print(\"\\nüìù Sample training example:\")\n",
        "print(f\"\\nID: {example['example_id']}\")\n",
        "print(f\"Style: {example['metadata']['target_style']}\")\n",
        "print(f\"Section: {example['metadata']['section_type']}\")\n",
        "print(f\"\\nOriginal (first 200 chars):\")\n",
        "print(f\"  {example['input']['original_section'][:200]}...\")\n",
        "print(f\"\\nTarget output (first 200 chars):\")\n",
        "print(f\"  {example['target_output'][:200]}...\")"
      ],
      "metadata": {
        "id": "preview_example"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Base Model with Unsloth\n",
        "\n",
        "We'll use LLaMA 3 8B with 4-bit quantization (QLoRA) for memory efficiency.\n",
        "\n",
        "**Why these settings:**\n",
        "- `load_in_4bit=True` ‚Üí Fits in GPU memory\n",
        "- `max_seq_length=2048` ‚Üí Enough for CV sections + context\n",
        "- LLaMA 3 8B ‚Üí Good balance of capability and trainability"
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048  # Enough for CV sections\n",
        "dtype = None  # Auto-detect (float16 for T4, bfloat16 for A100)\n",
        "load_in_4bit = True  # Use QLoRA for memory efficiency\n",
        "\n",
        "# Load LLaMA 3 8B\n",
        "print(\"‚è≥ Loading LLaMA 3 8B (this takes 1-2 minutes)...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",  # Pre-quantized for efficiency\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Base model loaded!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Configure LoRA Adapters\n",
        "\n",
        "**LoRA settings explained:**\n",
        "- `r=16` ‚Üí Rank (capacity) - higher = more capacity, more memory\n",
        "- `lora_alpha=32` ‚Üí Scaling factor (typically 2√ó rank)\n",
        "- `lora_dropout=0.05` ‚Üí Light regularization\n",
        "- `target_modules` ‚Üí Which layers to adapt (attention + MLP)"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank - balance between capacity and efficiency\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP layers\n",
        "    ],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Print trainable parameters\n",
        "def count_parameters(model):\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return trainable, total\n",
        "\n",
        "trainable, total = count_parameters(model)\n",
        "print(f\"\\nüìä Parameter count:\")\n",
        "print(f\"   Trainable: {trainable:,} ({trainable/total*100:.2f}%)\")\n",
        "print(f\"   Total: {total:,}\")\n",
        "print(f\"\\n‚úÖ LoRA adapters configured!\")"
      ],
      "metadata": {
        "id": "configure_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Format Training Data\n",
        "\n",
        "Convert our JSONL examples into the prompt format the model will learn."
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"### TASK\n",
        "Rewrite the following CV section according to the specified style and constraints.\n",
        "\n",
        "### ORIGINAL CV SECTION\n",
        "{original_section}\n",
        "\n",
        "### TARGET JOB CONTEXT\n",
        "{job_posting_excerpt}\n",
        "\n",
        "### INSTRUCTIONS\n",
        "{instructions}\n",
        "\n",
        "### REWRITTEN SECTION\n",
        "{target_output}\"\"\"\n",
        "\n",
        "# Add EOS token to signal end of generation\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def format_example(example):\n",
        "    \"\"\"Convert a training example to formatted text\"\"\"\n",
        "    text = PROMPT_TEMPLATE.format(\n",
        "        original_section=example['input']['original_section'],\n",
        "        job_posting_excerpt=example['input']['job_posting_excerpt'],\n",
        "        instructions=example['input']['instructions'],\n",
        "        target_output=example['target_output']\n",
        "    )\n",
        "    return {\"text\": text + EOS_TOKEN}\n",
        "\n",
        "# Preview formatted example\n",
        "sample = format_example(train_data[0])\n",
        "print(\"üìù Formatted training example (first 800 chars):\")\n",
        "print(sample['text'][:800])\n",
        "print(\"...\")"
      ],
      "metadata": {
        "id": "format_template"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Format all examples\n",
        "print(\"‚è≥ Formatting training data...\")\n",
        "train_formatted = [format_example(ex) for ex in train_data]\n",
        "val_formatted = [format_example(ex) for ex in val_data]\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_formatted)\n",
        "val_dataset = Dataset.from_list(val_formatted)\n",
        "\n",
        "print(f\"‚úÖ Datasets ready:\")\n",
        "print(f\"   Training: {len(train_dataset)} examples\")\n",
        "print(f\"   Validation: {len(val_dataset)} examples\")"
      ],
      "metadata": {
        "id": "create_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Configure Training\n",
        "\n",
        "**Training settings explained:**\n",
        "- `num_train_epochs=3` ‚Üí Train for 3 passes through data\n",
        "- `per_device_train_batch_size=2` ‚Üí Process 2 examples at a time\n",
        "- `gradient_accumulation_steps=4` ‚Üí Effective batch size = 8\n",
        "- `learning_rate=2e-4` ‚Üí Standard for LoRA fine-tuning\n",
        "- `warmup_steps=50` ‚Üí Gradual learning rate increase\n",
        "\n",
        "**Estimated time:** ~60-90 minutes on A100, ~2-3 hours on T4"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    # Output\n",
        "    output_dir=\"./outputs\",\n",
        "    \n",
        "    # Training duration\n",
        "    num_train_epochs=3,\n",
        "    \n",
        "    # Batch size (effective = per_device √ó accumulation = 2 √ó 4 = 8)\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=50,\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # Memory optimization\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    \n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Other\n",
        "    seed=42,\n",
        "    report_to=\"none\",  # Disable wandb\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration ready!\")\n",
        "print(f\"\\nüìä Training settings:\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Total steps: ~{len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
      ],
      "metadata": {
        "id": "training_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Don't pack multiple examples\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")"
      ],
      "metadata": {
        "id": "create_trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train! üöÄ\n",
        "\n",
        "This is the main training loop. Watch the loss decrease!\n",
        "\n",
        "**What to expect:**\n",
        "- Initial loss: ~2.5-3.0\n",
        "- Final loss: ~0.5-1.0 (good) or ~1.0-1.5 (acceptable)\n",
        "- Validation loss should track training loss (if much higher = overfitting)"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"üîß GPU: {gpu_stats.name}\")\n",
        "print(f\"üíæ Memory reserved: {start_gpu_memory} GB / {max_memory} GB\")\n",
        "print(f\"\\nüöÄ Starting training...\\n\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "pre_training_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ Training complete!\")\n",
        "print(f\"\\nüìä Final stats:\")\n",
        "print(f\"   Training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"   Training time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes\")\n",
        "print(f\"   Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check final memory usage\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nüíæ Peak GPU memory: {used_memory} GB / {max_memory} GB ({used_memory/max_memory*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "memory_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Save the Trained Adapter\n",
        "\n",
        "We save only the LoRA adapter (~50-100MB), not the full model (~16GB).\n",
        "\n",
        "This adapter can later be loaded on top of the base LLaMA 3 model."
      ],
      "metadata": {
        "id": "step8_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save adapter to Google Drive\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped folder\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "adapter_name = f\"cameleon_lora_{timestamp}\"\n",
        "save_path = os.path.join(OUTPUT_DIR, adapter_name)\n",
        "\n",
        "print(f\"üíæ Saving adapter to: {save_path}\")\n",
        "\n",
        "# Save LoRA adapter only (not full model)\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Check saved files\n",
        "saved_files = os.listdir(save_path)\n",
        "total_size = sum(os.path.getsize(os.path.join(save_path, f)) for f in saved_files)\n",
        "\n",
        "print(f\"\\n‚úÖ Adapter saved!\")\n",
        "print(f\"   Files: {len(saved_files)}\")\n",
        "print(f\"   Total size: {total_size / 1024 / 1024:.1f} MB\")\n",
        "print(f\"\\nüìÅ Saved files:\")\n",
        "for f in saved_files:\n",
        "    size = os.path.getsize(os.path.join(save_path, f)) / 1024 / 1024\n",
        "    print(f\"   {f}: {size:.2f} MB\")"
      ],
      "metadata": {
        "id": "save_adapter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Test Inference üß™\n",
        "\n",
        "Let's test the fine-tuned model with a sample CV section!"
      ],
      "metadata": {
        "id": "step9_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ Model ready for inference!\")"
      ],
      "metadata": {
        "id": "inference_mode"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt\n",
        "test_original = \"\"\"Managed customer service team of 8 people handling approximately 200 calls per day. \n",
        "Trained new hires on company procedures and phone etiquette. Reduced average call time from 6 minutes \n",
        "to 4 minutes by creating quick reference guides. Received employee of the month award twice in 2023.\"\"\"\n",
        "\n",
        "test_job = \"\"\"**Customer Experience Manager**\n",
        "Lead our customer support team to deliver exceptional service. You'll manage team performance,\n",
        "develop training programs, and drive efficiency improvements across support channels.\"\"\"\n",
        "\n",
        "# Create inference prompt (without target output)\n",
        "INFERENCE_TEMPLATE = \"\"\"### TASK\n",
        "Rewrite the following CV section according to the specified style and constraints.\n",
        "\n",
        "### ORIGINAL CV SECTION\n",
        "{original_section}\n",
        "\n",
        "### TARGET JOB CONTEXT\n",
        "{job_posting_excerpt}\n",
        "\n",
        "### INSTRUCTIONS\n",
        "{instructions}\n",
        "\n",
        "### REWRITTEN SECTION\n",
        "\"\"\"\n",
        "\n",
        "def test_style(style):\n",
        "    \"\"\"Generate a CV transformation for a given style\"\"\"\n",
        "    prompt = INFERENCE_TEMPLATE.format(\n",
        "        original_section=test_original,\n",
        "        job_posting_excerpt=test_job,\n",
        "        instructions=f\"Rewrite this CV experience section in {style} style. Preserve all facts exactly.\"\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    \n",
        "    # Decode and extract only the generated part\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated = full_output[len(prompt):].strip()\n",
        "    \n",
        "    return generated\n",
        "\n",
        "print(\"üß™ Testing inference...\\n\")"
      ],
      "metadata": {
        "id": "test_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test all 5 styles\n",
        "styles_to_test = ['confident', 'professional', 'concise', 'academic', 'playful']\n",
        "\n",
        "print(\"üìù ORIGINAL:\")\n",
        "print(test_original)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for style in styles_to_test:\n",
        "    print(f\"üéØ {style.upper()} STYLE:\")\n",
        "    result = test_style(style)\n",
        "    print(result)\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "test_all_styles"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Export for Deployment (Optional)\n",
        "\n",
        "If you want to merge the adapter with the base model for easier deployment:"
      ],
      "metadata": {
        "id": "step10_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Save merged model (full model, not just adapter)\n",
        "# This is larger (~16GB) but easier to deploy\n",
        "\n",
        "SAVE_MERGED = False  # Set to True if you want to save merged model\n",
        "\n",
        "if SAVE_MERGED:\n",
        "    merged_path = os.path.join(OUTPUT_DIR, f\"cameleon_merged_{timestamp}\")\n",
        "    print(f\"üíæ Saving merged model to: {merged_path}\")\n",
        "    print(\"‚è≥ This will take several minutes and ~16GB of space...\")\n",
        "    \n",
        "    # Save in 16-bit for deployment\n",
        "    model.save_pretrained_merged(\n",
        "        merged_path,\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "    )\n",
        "    print(f\"‚úÖ Merged model saved!\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipping merged model export. Set SAVE_MERGED = True to enable.\")"
      ],
      "metadata": {
        "id": "export_merged"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# üéâ Training Complete!\n",
        "\n",
        "## What you've accomplished:\n",
        "- ‚úÖ Fine-tuned LLaMA 3 8B with LoRA\n",
        "- ‚úÖ Trained on 840 style transformation examples\n",
        "- ‚úÖ Saved adapter to Google Drive\n",
        "- ‚úÖ Tested inference on all 5 styles\n",
        "\n",
        "## Your saved files:\n",
        "- **Adapter:** `CameleonCV/outputs/cameleon_lora_[timestamp]/`\n",
        "- **Size:** ~50-100MB\n",
        "\n",
        "## Next steps:\n",
        "1. **Evaluate** - Run systematic evaluation on test set\n",
        "2. **Compare** - Test base model (zero-shot) vs fine-tuned\n",
        "3. **Deploy** - Set up inference API or demo\n",
        "4. **Integrate** - Add Claude API for job relevance\n",
        "\n",
        "---\n",
        "\n",
        "**Questions?** Check your training loss curves and validation metrics to ensure the model learned well!"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}