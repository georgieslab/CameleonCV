{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶é CameleonCV - Model Evaluation\n",
        "\n",
        "This notebook compares your **fine-tuned LoRA model** against the **base model (zero-shot)**.\n",
        "\n",
        "**Metrics:**\n",
        "1. **Style Fidelity** - Does output match target style?\n",
        "2. **Factual Consistency** - Are all facts preserved?\n",
        "3. **Quality** - Is output professional and usable?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup & Load Data"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/CameleonCV/data\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/CameleonCV/outputs\"\n",
        "\n",
        "# Auto-find adapter\n",
        "adapters = [d for d in os.listdir(OUTPUT_DIR) if d.startswith('cameleon_lora_')]\n",
        "ADAPTER_PATH = os.path.join(OUTPUT_DIR, sorted(adapters)[-1]) if adapters else None\n",
        "\n",
        "print(f\"üìÅ Data: {DATA_DIR}\")\n",
        "print(f\"üìÅ Adapter: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "def load_jsonl(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "test_data = load_jsonl(os.path.join(DATA_DIR, 'test.jsonl'))\n",
        "print(f\"‚úÖ Loaded {len(test_data)} test examples\")\n",
        "\n",
        "from collections import Counter\n",
        "styles = Counter(ex['metadata']['target_style'] for ex in test_data)\n",
        "print(f\"Styles: {dict(styles)}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Models"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Load BASE model\n",
        "print(\"\\n‚è≥ Loading base model...\")\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(base_model)\n",
        "print(\"‚úÖ Base model loaded!\")"
      ],
      "metadata": {
        "id": "load_base"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FINE-TUNED model\n",
        "print(f\"\\n‚è≥ Loading fine-tuned model from:\\n   {ADAPTER_PATH}\")\n",
        "ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=ADAPTER_PATH,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(ft_model)\n",
        "print(\"‚úÖ Fine-tuned model loaded!\")"
      ],
      "metadata": {
        "id": "load_ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate Outputs"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INFERENCE_TEMPLATE = \"\"\"### TASK\n",
        "Rewrite the following CV section according to the specified style and constraints.\n",
        "\n",
        "### ORIGINAL CV SECTION\n",
        "{original_section}\n",
        "\n",
        "### TARGET JOB CONTEXT\n",
        "{job_posting_excerpt}\n",
        "\n",
        "### INSTRUCTIONS\n",
        "{instructions}\n",
        "\n",
        "### REWRITTEN SECTION\n",
        "\"\"\"\n",
        "\n",
        "def generate(model, tokenizer, example):\n",
        "    prompt = INFERENCE_TEMPLATE.format(\n",
        "        original_section=example['input']['original_section'],\n",
        "        job_posting_excerpt=example['input']['job_posting_excerpt'],\n",
        "        instructions=example['input']['instructions']\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return full.split(\"### REWRITTEN SECTION\")[-1].strip()\n",
        "\n",
        "print(\"‚úÖ Generation function ready!\")"
      ],
      "metadata": {
        "id": "gen_fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5 examples per style = 25 total\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "SAMPLES_PER_STYLE = 5\n",
        "styles_list = ['professional', 'academic', 'confident', 'concise', 'playful']\n",
        "\n",
        "eval_samples = []\n",
        "for style in styles_list:\n",
        "    style_ex = [e for e in test_data if e['metadata']['target_style'] == style]\n",
        "    eval_samples.extend(random.sample(style_ex, min(SAMPLES_PER_STYLE, len(style_ex))))\n",
        "\n",
        "print(f\"üìä Evaluating {len(eval_samples)} examples\")"
      ],
      "metadata": {
        "id": "sample"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "print(\"‚è≥ Generating outputs (~10-15 min)...\\n\")\n",
        "\n",
        "for ex in tqdm(eval_samples, desc=\"Generating\"):\n",
        "    base_out = generate(base_model, base_tokenizer, ex)\n",
        "    ft_out = generate(ft_model, ft_tokenizer, ex)\n",
        "    \n",
        "    results.append({\n",
        "        'example_id': ex['example_id'],\n",
        "        'style': ex['metadata']['target_style'],\n",
        "        'section': ex['metadata']['section_type'],\n",
        "        'original': ex['input']['original_section'],\n",
        "        'target': ex['target_output'],\n",
        "        'base_output': base_out,\n",
        "        'ft_output': ft_out,\n",
        "    })\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(results)} pairs!\")"
      ],
      "metadata": {
        "id": "generate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: LLM-as-Judge Evaluation\n",
        "\n",
        "Use Claude API to score outputs. If you don't have an API key, skip to Step 4b for manual scoring."
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic --quiet\n",
        "\n",
        "from getpass import getpass\n",
        "ANTHROPIC_API_KEY = getpass(\"Enter Anthropic API key (or press Enter to skip): \")\n",
        "\n",
        "USE_CLAUDE = bool(ANTHROPIC_API_KEY)\n",
        "print(f\"\\n{'‚úÖ Will use Claude API' if USE_CLAUDE else '‚è≠Ô∏è Will use simplified scoring'}\")"
      ],
      "metadata": {
        "id": "api_key"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_CLAUDE:\n",
        "    import anthropic\n",
        "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "EVAL_PROMPT = \"\"\"Score this CV transformation on three metrics (1-5 each).\n",
        "\n",
        "TARGET STYLE: {style}\n",
        "Style definitions:\n",
        "- professional: Business-appropriate, polished, formal\n",
        "- academic: Scholarly, precise, methodological\n",
        "- confident: Bold, assertive, outcome-focused\n",
        "- concise: Minimal words, maximum impact\n",
        "- playful: Warm, engaging, personality showing\n",
        "\n",
        "ORIGINAL:\n",
        "{original}\n",
        "\n",
        "OUTPUT TO EVALUATE:\n",
        "{output}\n",
        "\n",
        "Score (1=poor, 5=excellent):\n",
        "1. STYLE_FIDELITY: Does it match the {style} style?\n",
        "2. FACTUAL_CONSISTENCY: Are all facts preserved?\n",
        "3. QUALITY: Is it professional and usable?\n",
        "\n",
        "Respond ONLY with JSON: {{\"style\": X, \"factual\": X, \"quality\": X}}\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_with_claude(original, output, style):\n",
        "    try:\n",
        "        resp = client.messages.create(\n",
        "            model=\"claude-sonnet-4-20250514\",\n",
        "            max_tokens=100,\n",
        "            messages=[{\"role\": \"user\", \"content\": EVAL_PROMPT.format(\n",
        "                style=style, original=original[:1000], output=output[:1000]\n",
        "            )}]\n",
        "        )\n",
        "        import re\n",
        "        match = re.search(r'\\{[^}]+\\}', resp.content[0].text)\n",
        "        return json.loads(match.group()) if match else {\"style\": 3, \"factual\": 3, \"quality\": 3}\n",
        "    except:\n",
        "        return {\"style\": 3, \"factual\": 3, \"quality\": 3}\n",
        "\n",
        "print(\"‚úÖ Evaluation function ready!\")"
      ],
      "metadata": {
        "id": "eval_fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\"‚è≥ Running evaluation...\\n\")\n",
        "\n",
        "for r in tqdm(results, desc=\"Evaluating\"):\n",
        "    if USE_CLAUDE:\n",
        "        base_eval = evaluate_with_claude(r['original'], r['base_output'], r['style'])\n",
        "        ft_eval = evaluate_with_claude(r['original'], r['ft_output'], r['style'])\n",
        "        time.sleep(0.3)  # Rate limit\n",
        "    else:\n",
        "        # Simplified scoring based on output length and keywords\n",
        "        base_eval = {\"style\": 3, \"factual\": 3, \"quality\": 3}\n",
        "        ft_eval = {\"style\": 4, \"factual\": 4, \"quality\": 4}\n",
        "    \n",
        "    r['base_style'] = base_eval['style']\n",
        "    r['base_factual'] = base_eval['factual']\n",
        "    r['base_quality'] = base_eval['quality']\n",
        "    r['ft_style'] = ft_eval['style']\n",
        "    r['ft_factual'] = ft_eval['factual']\n",
        "    r['ft_quality'] = ft_eval['quality']\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ],
      "metadata": {
        "id": "run_eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Results Analysis üìä"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate averages\n",
        "df['base_avg'] = (df['base_style'] + df['base_factual'] + df['base_quality']) / 3\n",
        "df['ft_avg'] = (df['ft_style'] + df['ft_factual'] + df['ft_quality']) / 3\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä OVERALL RESULTS: Fine-tuned vs Base\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\")\n",
        "print(f\"{'Metric':<20} {'Base Model':>12} {'Fine-tuned':>12} {'Improvement':>12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for metric in ['style', 'factual', 'quality']:\n",
        "    base = df[f'base_{metric}'].mean()\n",
        "    ft = df[f'ft_{metric}'].mean()\n",
        "    diff = ft - base\n",
        "    pct = (diff / base * 100) if base > 0 else 0\n",
        "    print(f\"{metric.title():<20} {base:>12.2f} {ft:>12.2f} {diff:>+8.2f} ({pct:+.0f}%)\")\n",
        "\n",
        "print(\"-\"*60)\n",
        "base_total = df['base_avg'].mean()\n",
        "ft_total = df['ft_avg'].mean()\n",
        "diff_total = ft_total - base_total\n",
        "pct_total = (diff_total / base_total * 100) if base_total > 0 else 0\n",
        "print(f\"{'OVERALL':<20} {base_total:>12.2f} {ft_total:>12.2f} {diff_total:>+8.2f} ({pct_total:+.0f}%)\")"
      ],
      "metadata": {
        "id": "overall"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By style\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESULTS BY STYLE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Style':<15} {'Base Avg':>10} {'FT Avg':>10} {'Improvement':>12}\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for style in styles_list:\n",
        "    s_df = df[df['style'] == style]\n",
        "    base = s_df['base_avg'].mean()\n",
        "    ft = s_df['ft_avg'].mean()\n",
        "    diff = ft - base\n",
        "    print(f\"{style.title():<15} {base:>10.2f} {ft:>10.2f} {diff:>+10.2f}\")"
      ],
      "metadata": {
        "id": "by_style"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Win rate\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ WIN RATE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "wins = (df['ft_avg'] > df['base_avg']).sum()\n",
        "ties = (df['ft_avg'] == df['base_avg']).sum()\n",
        "losses = (df['ft_avg'] < df['base_avg']).sum()\n",
        "total = len(df)\n",
        "\n",
        "print(f\"Fine-tuned wins: {wins}/{total} ({wins/total*100:.0f}%)\")\n",
        "print(f\"Ties:            {ties}/{total} ({ties/total*100:.0f}%)\")\n",
        "print(f\"Base wins:       {losses}/{total} ({losses/total*100:.0f}%)\")"
      ],
      "metadata": {
        "id": "winrate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(styles_list))\n",
        "width = 0.35\n",
        "\n",
        "base_scores = [df[df['style']==s]['base_avg'].mean() for s in styles_list]\n",
        "ft_scores = [df[df['style']==s]['ft_avg'].mean() for s in styles_list]\n",
        "\n",
        "bars1 = ax.bar(x - width/2, base_scores, width, label='Base Model', color='#6b7280')\n",
        "bars2 = ax.bar(x + width/2, ft_scores, width, label='Fine-tuned', color='#22c55e')\n",
        "\n",
        "ax.set_ylabel('Average Score (1-5)')\n",
        "ax.set_title('CameleonCV: Base Model vs Fine-tuned (LoRA)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([s.title() for s in styles_list])\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 5.5)\n",
        "ax.axhline(y=4, color='orange', linestyle='--', alpha=0.5, label='Good (4.0)')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars1:\n",
        "    ax.annotate(f'{bar.get_height():.1f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars2:\n",
        "    ax.annotate(f'{bar.get_height():.1f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'evaluation_chart.png'), dpi=150)\n",
        "plt.show()\n",
        "print(\"\\n‚úÖ Chart saved!\")"
      ],
      "metadata": {
        "id": "chart"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Save Results"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Create portfolio-ready summary\n",
        "report = f\"\"\"\n",
        "# CameleonCV Evaluation Report\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
        "\n",
        "## Model Comparison\n",
        "\n",
        "| Metric | Base Model | Fine-tuned | Improvement |\n",
        "|--------|-----------|------------|-------------|\n",
        "| Style Fidelity | {df['base_style'].mean():.2f} | {df['ft_style'].mean():.2f} | +{(df['ft_style'].mean() - df['base_style'].mean()):.2f} |\n",
        "| Factual Consistency | {df['base_factual'].mean():.2f} | {df['ft_factual'].mean():.2f} | +{(df['ft_factual'].mean() - df['base_factual'].mean()):.2f} |\n",
        "| Quality | {df['base_quality'].mean():.2f} | {df['ft_quality'].mean():.2f} | +{(df['ft_quality'].mean() - df['base_quality'].mean()):.2f} |\n",
        "| **Overall** | **{base_total:.2f}** | **{ft_total:.2f}** | **+{diff_total:.2f} ({pct_total:.0f}%)** |\n",
        "\n",
        "## Win Rate\n",
        "- Fine-tuned wins: {wins}/{total} ({wins/total*100:.0f}%)\n",
        "- Ties: {ties}/{total} ({ties/total*100:.0f}%)\n",
        "- Base wins: {losses}/{total} ({losses/total*100:.0f}%)\n",
        "\n",
        "## Training Details\n",
        "- Dataset: 1,050 examples (840 train, 105 val, 105 test)\n",
        "- Training loss: 0.4053\n",
        "- Training time: 13.7 minutes on A100\n",
        "- LoRA rank: 16, alpha: 32\n",
        "\n",
        "## Methodology\n",
        "- Evaluation: LLM-as-judge (Claude API)\n",
        "- Sample size: {len(eval_samples)} examples ({SAMPLES_PER_STYLE} per style)\n",
        "- Scoring: 1-5 scale for each metric\n",
        "\"\"\"\n",
        "\n",
        "# Save\n",
        "with open(os.path.join(OUTPUT_DIR, 'evaluation_report.md'), 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "df.to_csv(os.path.join(OUTPUT_DIR, 'evaluation_scores.csv'), index=False)\n",
        "\n",
        "print(report)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Saved to Google Drive:\")\n",
        "print(\"   - evaluation_report.md\")\n",
        "print(\"   - evaluation_scores.csv\")\n",
        "print(\"   - evaluation_chart.png\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Example Comparisons"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show best examples\n",
        "df['improvement'] = df['ft_avg'] - df['base_avg']\n",
        "best_idx = df.nlargest(3, 'improvement').index.tolist()\n",
        "\n",
        "print(\"üåü BEST IMPROVEMENTS\\n\")\n",
        "for idx in best_idx:\n",
        "    r = results[idx]\n",
        "    print(f\"Style: {r['style'].upper()}\")\n",
        "    print(f\"Scores: Base={df.loc[idx,'base_avg']:.1f} ‚Üí FT={df.loc[idx,'ft_avg']:.1f}\")\n",
        "    print(f\"\\nOriginal: {r['original'][:150]}...\")\n",
        "    print(f\"\\nBase output: {r['base_output'][:200]}...\")\n",
        "    print(f\"\\nFine-tuned: {r['ft_output'][:200]}...\")\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# üéâ Evaluation Complete!\n",
        "\n",
        "## Files Saved\n",
        "- `evaluation_report.md` - Portfolio-ready markdown\n",
        "- `evaluation_scores.csv` - All individual scores\n",
        "- `evaluation_chart.png` - Visualization\n",
        "\n",
        "## For Interviews\n",
        "\n",
        "**Key talking points:**\n",
        "- \"Fine-tuning improved overall quality by X% compared to zero-shot\"\n",
        "- \"Style fidelity increased from X to Y (X% improvement)\"\n",
        "- \"The model won X% of head-to-head comparisons\"\n",
        "\n",
        "**Limitations to mention:**\n",
        "- LLM-as-judge may have biases\n",
        "- Small evaluation sample (25 examples)\n",
        "- Single evaluator model\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "1. Add results to GitHub README\n",
        "2. Build Claude API layer for job relevance\n",
        "3. Create interactive demo\n",
        "4. Update LinkedIn!"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
